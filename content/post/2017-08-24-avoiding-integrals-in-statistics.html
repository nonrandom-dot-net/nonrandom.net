---
title: Avoiding integrals in statistics
author: David J. Harris
date: '2017-08-22'
categories:
  - Approximation
  - Integration
  - Latent variables
  - Monte Carlo
  - Statistics
  - Variational methods
slug: avoiding-integrals-in-statistics
---



<div id="why-integrate" class="section level1">
<h1>Why integrate?</h1>
<p>independent random events whose intensity is drawn from a gamma distribution.</p>
<p>, so we can just use <code>dnbinom</code> in R, or <code>scipy.stats.nbinom.pmf</code> in python. But what if</p>
<pre class="r"><code>set.seed(1)
N = 1000
shape = 4
rate = 1
xmax = 12
posterior_height = .04</code></pre>
<pre class="r"><code>curve(dgamma(x, shape, rate), to = xmax, yaxs = &quot;i&quot;, bty = &quot;l&quot;,
      n = 1000, ylim = c(0, 1), ylab = &quot;Probability&quot;)
prob = 1 / (1 + 1/rate)
lines(0:100, dnbinom(0:100, size = shape, prob = prob), col = &quot;darkred&quot;,
      type = &quot;h&quot;)
lines(0, dnbinom(0, size = shape, prob = prob), type = &quot;h&quot;, col = 2, lwd = 3)</code></pre>
<p><img src="/post/2017-08-24-avoiding-integrals-in-statistics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>curve(dgamma(x, shape, rate), to = xmax, yaxs = &quot;i&quot;, bty = &quot;l&quot;,
      n = 1000, ylim = c(0, 1), ylab = &quot;Probability&quot;)
curve(dpois(0, x), to = xmax, add = TRUE, col = 2)
arrows(5, .4, 2, .2, lwd = 2, length = .125)</code></pre>
<p><img src="/post/2017-08-24-avoiding-integrals-in-statistics_files/figure-html/unnamed-chunk-3-1.png" width="672" /> If the intensity has a typical value lik 5 or 10, we <em>could</em> still get unlucky and get nothing, but it wouldn’t be very likely. Alternatively, if the intensity is less than 1, then we wouldn’t expect anything, but those extremely low intensities are pretty rare. And what about all the values in between?</p>
<pre class="r"><code>likelihood = function(lambda){
  dpois(0, lambda) * dgamma(lambda, shape, rate)
}
curve(likelihood, to = xmax, yaxs = &quot;i&quot;, bty = &quot;l&quot;,
      n = 1000, ylim = c(0, posterior_height), ylab = &quot;Probability&quot;)</code></pre>
<p><img src="/post/2017-08-24-avoiding-integrals-in-statistics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>This gives us all the ways that we could have gotten zero, weighted by their probability. If we want the total probability associated with zero, we “just” need to add up those weights. Adding up all the possible weights gives us this integral</p>
<p><span class="math display">\[\int_0^\infty p(k=0|\lambda)~~p(\lambda|\alpha\beta)~~d\lambda,\]</span></p>
<p>where <span class="math inline">\(p(k|\lambda)=\frac{\lambda^k e^{-\lambda}}{k!}\)</span> (i.e. the Poisson likelihood) and <span class="math inline">\(p(\lambda|\alpha,\beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta \lambda}\)</span> (i.e. the gamma likelihood).</p>
<p>Fortunately, I chose an example where this integral is known: it’s “just” the negative binomial distribution’s probability mass function, i.e.</p>
<p><span class="math display">\[p(k|p,\alpha)={{k + \alpha - 1}\choose{k}} (1-p)^\alpha p^k.\]</span></p>
<p>where <span class="math inline">\(p=1 / (1 + 1/\beta).\)</span> If you happen to work exclusively with special cases (called conjugate distributions) whose integrals you can look up in a textbook, then this is as far as you need to read. But this strategy doesn’t work in general: if we have uncertainty about the values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, or if the intensities come from some other distribution like the lognormal, or if we wanted to include additional prior information about <span class="math inline">\(\lambda\)</span>, or if we wanted to integrate out <span class="math inline">\(\lambda\)</span> and another parameter at the same time, we’d be stuck with something we couldn’t compute.</p>
<p>In short, we’ll have to use some kind of approximation pretty often.</p>
</div>
<div id="how-to-avoid-integrating" class="section level1">
<h1>How to avoid integrating</h1>
<div id="numerical-integration-part-1-newtoncotes" class="section level2">
<h2>Numerical integration, part 1: Newton–Cotes</h2>
<p>If you’ve taken a calculus class that covers integration, you might remember “the rectangle rule”, “the trapezoid rule”, and/or “Simpson’s rule”. These methods all estimate the area under a curve by examining the height of the curve (and possibly its slope and other derivatives) at a series of closely-spaced points. This gives an approximation of the curve’s shape; calculating the area under our approximation gives us an estimate of the area under the original curve.</p>
<p>This is rarely the best method to use in statistics (especially when integrating over multiple dimensions at once), but it is one of the easiest ones to understand.</p>
<pre class="r"><code>delta = 0.5
lambda_sequence = seq(0, xmax, delta)
rect_lik = dpois(0, lambda_sequence) * dgamma(lambda_sequence, shape, rate)
rect_xmin = lambda_sequence - delta / 2
rect_xmax = lambda_sequence + delta / 2

plot(NULL, xlim = c(0, xmax), ylim = c(0, posterior_height))
rect(rect_xmin, 0, rect_xmax, rect_lik)
curve(dpois(0, x) * dgamma(x, shape, rate), add = TRUE)</code></pre>
<p><img src="/post/2017-08-24-avoiding-integrals-in-statistics_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>sum(rect_lik) * delta</code></pre>
<pre><code>## [1] 0.06256784</code></pre>
<pre class="r"><code>dnbinom(0, size = shape, prob = prob)</code></pre>
<pre><code>## [1] 0.0625</code></pre>
</div>
<div id="monte-carlo" class="section level2">
<h2>Monte Carlo</h2>
<p>This is of the most commonly-used approaches, especially in Bayesian statistics. The underlying idea is straightforward: Instead using <em>all</em> possible values for <span class="math inline">\(\lambda\)</span>, work with a representative sample.</p>
<p>Here, we know what the distribution of <span class="math inline">\(\lambda\)</span> is, so it’s easy to generate 10 million representative samples and determine their Poisson likelihoods. The answer we get this way agrees closely with the correct answer, generated by the negative binomial distribution.</p>
<pre class="r"><code>N = 10000000
random_lambdas = rgamma(N, shape, rate)
mean(dpois(0, random_lambdas))</code></pre>
<pre><code>## [1] 0.06249188</code></pre>
<pre class="r"><code>dnbinom(0, size = shape, prob = prob)</code></pre>
<pre><code>## [1] 0.0625</code></pre>
<p>In practice, it’s usally hard to sample from the parameter distribution, and fancier versions of this approach are needed.</p>
</div>
<div id="laplace-approximation" class="section level2">
<h2>Laplace approximation</h2>
<pre class="r"><code>log_likelihood_log_scale = function(log_lambda) {
  log(likelihood(exp(log_lambda)))
}
mle_log = optimize(log_likelihood_log_scale, c(-10, 10), maximum = TRUE)$maximum
sigma_log = 1/sqrt(-numDeriv::hessian(log_likelihood_log_scale, mle_log))

log_lambda_sequence = seq(log(1/xmax), log(xmax), length = 1000)

laplace_amount = likelihood(exp(mle_log)) / dnorm(mle_log, mle_log, sigma_log)
laplace_amount2 = likelihood(exp(mle_log)) / dlnorm(exp(mle_log), mle_log, sigma_log)

curve(laplace_amount * dnorm(log(x), mle_log, sigma_log), add = FALSE, n = 1000, 
      from = .01, to = xmax, log = &quot;x&quot;, ylab = &quot;density&quot;,
      ylim = c(0, posterior_height))
curve(likelihood(x), add = TRUE, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2017-08-24-avoiding-integrals-in-statistics_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code># Natural scale
curve(laplace_amount2 * dlnorm(x, mle_log, sigma_log), add = FALSE, n = 1000, 
      from = .01, to = xmax, ylab = &quot;density&quot;,
      ylim = c(0, posterior_height))
curve(likelihood(x), add = TRUE, col = &quot;blue&quot;)</code></pre>
<p><img src="/post/2017-08-24-avoiding-integrals-in-statistics_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<pre class="r"><code>laplace_amount2</code></pre>
<pre><code>## [1] 0.06079395</code></pre>
<ul>
<li>Wald approximation, often used for margin of error</li>
<li>Used by <code>glm</code> function for confidence intervals</li>
</ul>
</div>
<div id="numerical-integration-part-2-gaussian-quadrature-gauss-hermite-quadrature" class="section level2">
<h2>Numerical integration, part 2: Gaussian quadrature &amp; Gauss-Hermite quadrature</h2>
<pre class="r"><code>library(fastGHQuad)</code></pre>
<pre><code>## Loading required package: Rcpp</code></pre>
<pre class="r"><code>sigma = 1/sqrt(-numDeriv::hessian(
  function(x)log_likelihood_log_scale(exp(x)), 
  exp(mle_log)))

ghquad = function(i){
  aghQuad(
    function(x){
      sapply(
        x,
        function(lambda){
          if (lambda &gt; 0) {
            likelihood(lambda)
          }else{
            0
          }
        }
      )
    }, 
    muHat = exp(mle_log), 
    sigmaHat = sigma_log, 
    rule = gaussHermiteData(i))
}

plot(sapply(1:50, ghquad))
abline(h = dnbinom(0, size = shape, prob = prob))</code></pre>
<p><img src="/post/2017-08-24-avoiding-integrals-in-statistics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="variational-approximation" class="section level2">
<h2>Variational approximation</h2>
</div>
<div id="numerical-integration-part-3-bayesian-quadrature" class="section level2">
<h2>Numerical integration, part 3: Bayesian Quadrature</h2>
</div>
</div>
